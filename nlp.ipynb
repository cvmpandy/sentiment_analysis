{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q5K9cdIwg2Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e267a716-aeb9-4f03-b144-8856971024a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz...\n",
            "Download complete.\n",
            "Extracting rt-polaritydata.tar.gz...\n",
            "Extraction complete. Files extracted to rt-polaritydata\n",
            "Extracted files: ['rt-polaritydata.README.1.0.txt', 'rt-polaritydata']\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
        "dataset_folder = \"rt-polaritydata\"\n",
        "\n",
        "# Function to download the dataset\n",
        "def download_dataset(url, download_path):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(f\"Downloading dataset from {url}...\")\n",
        "        urllib.request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "# Function to extract the tar.gz file\n",
        "def extract_dataset(tar_path, extract_to):\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"Extracting {tar_path}...\")\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "        print(f\"Extraction complete. Files extracted to {extract_to}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n",
        "\n",
        "# Download and extract dataset\n",
        "download_path = \"rt-polaritydata.tar.gz\"\n",
        "download_dataset(url, download_path)\n",
        "extract_dataset(download_path, dataset_folder)\n",
        "\n",
        "# List the extracted files\n",
        "print(f\"Extracted files: {os.listdir(dataset_folder)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd rt-polaritydata/rt-polaritydata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q1x6D8djx3A",
        "outputId": "504f2431-167d-4d1c-974c-09f728c30081"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rt-polaritydata/rt-polaritydata/rt-polaritydata/rt-polaritydata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyKlOZbij4pR",
        "outputId": "3b1551f9-4e3e-47dd-e510-fe9c2a717a9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt-polarity.neg  rt-polarity.pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c_eeSmTjdKV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Colab, install TensorFlow Hub and TensorFlow\n",
        "!pip install tensorflow tensorflow-hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR9qv18wlxu2",
        "outputId": "6d08279e-5de0-4cb7-bc17-47040344278b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.66.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# 1. Load the data from files\n",
        "def load_data(positive_file, negative_file):\n",
        "    # Reading positive and negative data\n",
        "    with open(positive_file, 'r', encoding='latin-1') as pos_file:\n",
        "        positive_data = pos_file.readlines()\n",
        "\n",
        "    with open(negative_file, 'r', encoding='latin-1') as neg_file:\n",
        "        negative_data = neg_file.readlines()\n",
        "\n",
        "    # Strip leading/trailing whitespaces\n",
        "    positive_data = [line.strip() for line in positive_data]\n",
        "    negative_data = [line.strip() for line in negative_data]\n",
        "\n",
        "    return positive_data, negative_data\n",
        "\n",
        "# 2. Label the data\n",
        "def prepare_dataframe(positive_data, negative_data):\n",
        "    # Create dataframes for positive and negative data\n",
        "    pos_df = pd.DataFrame({'text': positive_data, 'label': 1})  # 1 for positive\n",
        "    neg_df = pd.DataFrame({'text': negative_data, 'label': 0})  # 0 for negative\n",
        "\n",
        "    # Concatenate both dataframes\n",
        "    data = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    data = shuffle(data).reset_index(drop=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 3. Split data into training, validation, and test sets\n",
        "def split_data(data):\n",
        "    # Split into training (4000 each), validation (500 each), and test sets (831 each)\n",
        "    train_pos = data[data['label'] == 1][:4000]\n",
        "    train_neg = data[data['label'] == 0][:4000]\n",
        "    val_pos = data[data['label'] == 1][4000:4500]\n",
        "    val_neg = data[data['label'] == 0][4000:4500]\n",
        "    test_pos = data[data['label'] == 1][4500:]\n",
        "    test_neg = data[data['label'] == 0][4500:]\n",
        "\n",
        "    # Combine positive and negative for each set\n",
        "    train_data = pd.concat([train_pos, train_neg], ignore_index=True)\n",
        "    val_data = pd.concat([val_pos, val_neg], ignore_index=True)\n",
        "    test_data = pd.concat([test_pos, test_neg], ignore_index=True)\n",
        "\n",
        "    # Shuffle each set to mix positives and negatives\n",
        "    train_data = shuffle(train_data).reset_index(drop=True)\n",
        "    val_data = shuffle(val_data).reset_index(drop=True)\n",
        "    test_data = shuffle(test_data).reset_index(drop=True)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# 4. Vectorize the text using TF-IDF\n",
        "def vectorize_data(train_data, val_data, test_data):\n",
        "    # Use TF-IDF vectorizer to convert text to numerical features\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "    # Fit on training data and transform\n",
        "    X_train = vectorizer.fit_transform(train_data['text'])\n",
        "    X_val = vectorizer.transform(val_data['text'])\n",
        "    X_test = vectorizer.transform(test_data['text'])\n",
        "\n",
        "    # Extract labels\n",
        "    y_train = train_data['label']\n",
        "    y_val = val_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "# Main function to run preprocessing\n",
        "def preprocess_data(positive_file, negative_file):\n",
        "    # Load the data\n",
        "    positive_data, negative_data = load_data(positive_file, negative_file)\n",
        "\n",
        "    # Prepare and shuffle the dataframe\n",
        "    data = prepare_dataframe(positive_data, negative_data)\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "    # Vectorize the data using TF-IDF\n",
        "    # X_train, X_val, X_test, y_train, y_val, y_test = vectorize_data(train_data, val_data, test_data)\n",
        "\n",
        "    # return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Paths to the data files\n",
        "positive_file = 'rt-polarity.pos'\n",
        "negative_file = 'rt-polarity.neg'\n",
        "\n",
        "# Preprocess the data\n",
        "train_data, val_data, test_data = preprocess_data(positive_file, negative_file)\n",
        "\n",
        "# You can now use X_train, X_val, X_test, y_train, y_val, y_test for model training and evaluation.\n"
      ],
      "metadata": {
        "id": "lUYIAYF2j6NN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n"
      ],
      "metadata": {
        "id": "CWCIyVm6T4gJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ELMo model from TensorFlow Hub\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# Function to get ELMo embeddings\n",
        "# def elmo_embedding(text):\n",
        "#     embeddings = elmo.signatures[\"default\"](tf.constant([text]))[\"elmo\"]\n",
        "#     return tf.reduce_mean(embeddings, axis=1).numpy()\n",
        "\n",
        "# # Test embedding for one sentence\n",
        "# print(elmo_embedding(\"This movie was absolutely amazing!\"))\n",
        "\n",
        "def elmo_embedding(sentences):\n",
        "    # ELMo expects a list of sentences\n",
        "    embeddings = elmo.signatures['default'](tf.convert_to_tensor(sentences))['elmo']\n",
        "    # Averaging the embeddings for each sentence\n",
        "    return np.array([np.mean(embedding, axis=0) for embedding in embeddings])\n",
        "\n",
        "sample_sentences = [\"This is a great movie.\", \"I didn't enjoy this film.\"]\n",
        "elmo_embeddings = elmo_embedding(sample_sentences)\n",
        "print(elmo_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap05v5LKUvy_",
        "outputId": "81ddd6a0-320c-46d2-c104-b06f9963b54f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_to_elmo_embeddings(data):\n",
        "#     embeddings = [elmo_embedding(sentence) for sentence in data['text']]\n",
        "#     return np.vstack(embeddings)\n",
        "\n",
        "def convert_to_elmo_embeddings(data):\n",
        "    embeddings = elmo_embedding(data['text'].tolist())\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "\n",
        "# Convert the datasets\n",
        "X_train = convert_to_elmo_embeddings(train_data)\n",
        "X_val = convert_to_elmo_embeddings(val_data)\n",
        "X_test = convert_to_elmo_embeddings(test_data)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_data['label'].values\n",
        "y_val = val_data['label'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "# Check shape of embeddings\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_val shape: {X_val.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n"
      ],
      "metadata": {
        "id": "WngpXHmjW06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b37e715-9dfc-443b-f238-ebefa93e45c7"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (8000, 1024)\n",
            "X_val shape: (1000, 1024)\n",
            "X_test shape: (1662, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Model training\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Validation\n",
        "val_preds = clf.predict(X_val)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_preds)}\")\n",
        "\n",
        "# Testing\n",
        "test_preds = clf.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, test_preds)}\")\n",
        "print(classification_report(y_test, test_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3YdnztJlHQy",
        "outputId": "bfaacc9c-2f03-468e-e0ef-163379aee14a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.791\n",
            "Test Accuracy: 0.8074608904933814\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       831\n",
            "           1       0.81      0.80      0.81       831\n",
            "\n",
            "    accuracy                           0.81      1662\n",
            "   macro avg       0.81      0.81      0.81      1662\n",
            "weighted avg       0.81      0.81      0.81      1662\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Krb8GD4loNvM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}