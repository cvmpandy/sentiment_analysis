{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "q5K9cdIwg2Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e267a716-aeb9-4f03-b144-8856971024a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading dataset from https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz...\n",
            "Download complete.\n",
            "Extracting rt-polaritydata.tar.gz...\n",
            "Extraction complete. Files extracted to rt-polaritydata\n",
            "Extracted files: ['rt-polaritydata.README.1.0.txt', 'rt-polaritydata']\n"
          ]
        }
      ],
      "source": [
        "import urllib.request\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# URL of the dataset\n",
        "url = \"https://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
        "dataset_folder = \"rt-polaritydata\"\n",
        "\n",
        "# Function to download the dataset\n",
        "def download_dataset(url, download_path):\n",
        "    if not os.path.exists(download_path):\n",
        "        print(f\"Downloading dataset from {url}...\")\n",
        "        urllib.request.urlretrieve(url, download_path)\n",
        "        print(\"Download complete.\")\n",
        "    else:\n",
        "        print(\"Dataset already downloaded.\")\n",
        "\n",
        "# Function to extract the tar.gz file\n",
        "def extract_dataset(tar_path, extract_to):\n",
        "    if not os.path.exists(extract_to):\n",
        "        print(f\"Extracting {tar_path}...\")\n",
        "        with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "            tar.extractall(path=extract_to)\n",
        "        print(f\"Extraction complete. Files extracted to {extract_to}\")\n",
        "    else:\n",
        "        print(\"Dataset already extracted.\")\n",
        "\n",
        "# Download and extract dataset\n",
        "download_path = \"rt-polaritydata.tar.gz\"\n",
        "download_dataset(url, download_path)\n",
        "extract_dataset(download_path, dataset_folder)\n",
        "\n",
        "# List the extracted files\n",
        "print(f\"Extracted files: {os.listdir(dataset_folder)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd rt-polaritydata/rt-polaritydata"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Q1x6D8djx3A",
        "outputId": "504f2431-167d-4d1c-974c-09f728c30081"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/rt-polaritydata/rt-polaritydata/rt-polaritydata/rt-polaritydata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyKlOZbij4pR",
        "outputId": "3b1551f9-4e3e-47dd-e510-fe9c2a717a9a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rt-polarity.neg  rt-polarity.pos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0c_eeSmTjdKV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In Colab, install TensorFlow Hub and TensorFlow\n",
        "!pip install tensorflow tensorflow-hub\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rR9qv18wlxu2",
        "outputId": "6d08279e-5de0-4cb7-bc17-47040344278b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: tensorflow-hub in /usr/local/lib/python3.10/dist-packages (0.16.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.66.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: tf-keras>=2.14.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow-hub) (2.15.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.32.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.5.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# 1. Load the data from files\n",
        "def load_data(positive_file, negative_file):\n",
        "    # Reading positive and negative data\n",
        "    with open(positive_file, 'r', encoding='latin-1') as pos_file:\n",
        "        positive_data = pos_file.readlines()\n",
        "\n",
        "    with open(negative_file, 'r', encoding='latin-1') as neg_file:\n",
        "        negative_data = neg_file.readlines()\n",
        "\n",
        "    # Strip leading/trailing whitespaces\n",
        "    positive_data = [line.strip() for line in positive_data]\n",
        "    negative_data = [line.strip() for line in negative_data]\n",
        "\n",
        "    return positive_data, negative_data\n",
        "\n",
        "# 2. Label the data\n",
        "def prepare_dataframe(positive_data, negative_data):\n",
        "    # Create dataframes for positive and negative data\n",
        "    pos_df = pd.DataFrame({'text': positive_data, 'label': 1})  # 1 for positive\n",
        "    neg_df = pd.DataFrame({'text': negative_data, 'label': 0})  # 0 for negative\n",
        "\n",
        "    # Concatenate both dataframes\n",
        "    data = pd.concat([pos_df, neg_df], ignore_index=True)\n",
        "\n",
        "    # Shuffle the dataset\n",
        "    data = shuffle(data).reset_index(drop=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# 3. Split data into training, validation, and test sets\n",
        "def split_data(data):\n",
        "    # Split into training (4000 each), validation (500 each), and test sets (831 each)\n",
        "    train_pos = data[data['label'] == 1][:4000]\n",
        "    train_neg = data[data['label'] == 0][:4000]\n",
        "    val_pos = data[data['label'] == 1][4000:4500]\n",
        "    val_neg = data[data['label'] == 0][4000:4500]\n",
        "    test_pos = data[data['label'] == 1][4500:]\n",
        "    test_neg = data[data['label'] == 0][4500:]\n",
        "\n",
        "    # Combine positive and negative for each set\n",
        "    train_data = pd.concat([train_pos, train_neg], ignore_index=True)\n",
        "    val_data = pd.concat([val_pos, val_neg], ignore_index=True)\n",
        "    test_data = pd.concat([test_pos, test_neg], ignore_index=True)\n",
        "\n",
        "    # Shuffle each set to mix positives and negatives\n",
        "    train_data = shuffle(train_data).reset_index(drop=True)\n",
        "    val_data = shuffle(val_data).reset_index(drop=True)\n",
        "    test_data = shuffle(test_data).reset_index(drop=True)\n",
        "\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# 4. Vectorize the text using TF-IDF\n",
        "def vectorize_data(train_data, val_data, test_data):\n",
        "    # Use TF-IDF vectorizer to convert text to numerical features\n",
        "    vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
        "\n",
        "    # Fit on training data and transform\n",
        "    X_train = vectorizer.fit_transform(train_data['text'])\n",
        "    X_val = vectorizer.transform(val_data['text'])\n",
        "    X_test = vectorizer.transform(test_data['text'])\n",
        "\n",
        "    # Extract labels\n",
        "    y_train = train_data['label']\n",
        "    y_val = val_data['label']\n",
        "    y_test = test_data['label']\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "\n",
        "# Main function to run preprocessing\n",
        "def preprocess_data(positive_file, negative_file):\n",
        "    # Load the data\n",
        "    positive_data, negative_data = load_data(positive_file, negative_file)\n",
        "\n",
        "    # Prepare and shuffle the dataframe\n",
        "    data = prepare_dataframe(positive_data, negative_data)\n",
        "\n",
        "    # Split the data into training, validation, and test sets\n",
        "    train_data, val_data, test_data = split_data(data)\n",
        "\n",
        "    # Vectorize the data using TF-IDF\n",
        "    # X_train, X_val, X_test, y_train, y_val, y_test = vectorize_data(train_data, val_data, test_data)\n",
        "\n",
        "    # return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "    return train_data, val_data, test_data\n",
        "\n",
        "# Paths to the data files\n",
        "positive_file = 'rt-polarity.pos'\n",
        "negative_file = 'rt-polarity.neg'\n",
        "\n",
        "# Preprocess the data\n",
        "train_data, val_data, test_data = preprocess_data(positive_file, negative_file)\n",
        "\n",
        "# You can now use X_train, X_val, X_test, y_train, y_val, y_test for model training and evaluation.\n"
      ],
      "metadata": {
        "id": "lUYIAYF2j6NN"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, LSTM, Bidirectional\n"
      ],
      "metadata": {
        "id": "CWCIyVm6T4gJ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the ELMo model from TensorFlow Hub\n",
        "elmo = hub.load(\"https://tfhub.dev/google/elmo/3\")\n",
        "\n",
        "# Function to get ELMo embeddings\n",
        "# def elmo_embedding(text):\n",
        "#     embeddings = elmo.signatures[\"default\"](tf.constant([text]))[\"elmo\"]\n",
        "#     return tf.reduce_mean(embeddings, axis=1).numpy()\n",
        "\n",
        "# # Test embedding for one sentence\n",
        "# print(elmo_embedding(\"This movie was absolutely amazing!\"))\n",
        "\n",
        "def elmo_embedding(sentences):\n",
        "    # ELMo expects a list of sentences\n",
        "    embeddings = elmo.signatures['default'](tf.convert_to_tensor(sentences))['elmo']\n",
        "    # Averaging the embeddings for each sentence\n",
        "    # return np.array([np.mean(embedding, axis=0) for embedding in embeddings])\n",
        "    return np.array(embeddings)\n",
        "\n",
        "sample_sentences = [\"This is a great movie.\", \"I didn't enjoy this film.\"]\n",
        "elmo_embeddings = elmo_embedding(sample_sentences)\n",
        "print(elmo_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ap05v5LKUvy_",
        "outputId": "817b09b4-82f3-49b8-d7b8-ee8b0a70e3e2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def convert_to_elmo_embeddings(data):\n",
        "#     embeddings = [elmo_embedding(sentence) for sentence in data['text']]\n",
        "#     return np.vstack(embeddings)\n",
        "\n",
        "def convert_to_elmo_embeddings(data):\n",
        "    embeddings = elmo_embedding(data['text'].tolist())\n",
        "    return embeddings\n",
        "\n",
        "\n",
        "\n",
        "# Convert the datasets\n",
        "X_train = convert_to_elmo_embeddings(train_data)\n",
        "X_val = convert_to_elmo_embeddings(val_data)\n",
        "X_test = convert_to_elmo_embeddings(test_data)\n",
        "\n",
        "# Extract labels\n",
        "y_train = train_data['label'].values\n",
        "y_val = val_data['label'].values\n",
        "y_test = test_data['label'].values\n",
        "\n",
        "# Check shape of embeddings\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_val shape: {X_val.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n"
      ],
      "metadata": {
        "id": "WngpXHmjW06b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f6c79b4-49eb-476e-f20c-838c7ca188ee"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: (8000, 59, 1024)\n",
            "X_val shape: (1000, 55, 1024)\n",
            "X_test shape: (1662, 54, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "\n",
        "# Model training\n",
        "clf = LogisticRegression()\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "# Validation\n",
        "val_preds = clf.predict(X_val)\n",
        "print(f\"Validation Accuracy: {accuracy_score(y_val, val_preds)}\")\n",
        "\n",
        "# Testing\n",
        "test_preds = clf.predict(X_test)\n",
        "print(f\"Test Accuracy: {accuracy_score(y_test, test_preds)}\")\n",
        "print(classification_report(y_test, test_preds))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3YdnztJlHQy",
        "outputId": "bfaacc9c-2f03-468e-e0ef-163379aee14a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 0.791\n",
            "Test Accuracy: 0.8074608904933814\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.82      0.81       831\n",
            "           1       0.81      0.80      0.81       831\n",
            "\n",
            "    accuracy                           0.81      1662\n",
            "   macro avg       0.81      0.81      0.81      1662\n",
            "weighted avg       0.81      0.81      0.81      1662\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import svm\n",
        "print(\"\\nTraining SVM Classifier...\")\n",
        "svm_clf = svm.SVC(kernel='linear', C=1.0)\n",
        "svm_clf.fit(X_train, y_train)\n",
        "\n",
        "# Validation accuracy for SVM\n",
        "val_preds_svm = svm_clf.predict(X_val)\n",
        "print(f\"SVM Validation Accuracy: {accuracy_score(y_val, val_preds_svm)}\")\n",
        "\n",
        "# Test accuracy for SVM\n",
        "test_preds_svm = svm_clf.predict(X_test)\n",
        "print(f\"SVM Test Accuracy: {accuracy_score(y_test, test_preds_svm)}\")\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_test, test_preds_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Krb8GD4loNvM",
        "outputId": "b2c17e8a-37fe-4d55-ce81-0ee62e41a963"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training SVM Classifier...\n",
            "SVM Validation Accuracy: 0.787\n",
            "SVM Test Accuracy: 0.8038507821901324\n",
            "SVM Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.82      0.81       831\n",
            "           1       0.82      0.79      0.80       831\n",
            "\n",
            "    accuracy                           0.80      1662\n",
            "   macro avg       0.80      0.80      0.80      1662\n",
            "weighted avg       0.80      0.80      0.80      1662\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout\n",
        "# from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# 1. Build BiLSTM Model\n",
        "def build_bilstm_model(input_shape):\n",
        "    model = Sequential()\n",
        "\n",
        "    # Add a bidirectional LSTM layer\n",
        "    model.add(Bidirectional(LSTM(units=128, return_sequences=False), input_shape=input_shape))\n",
        "\n",
        "    # Add a dropout layer to avoid overfitting\n",
        "    model.add(Dropout(0.5))\n",
        "\n",
        "    # Add a dense layer for output\n",
        "    model.add(Dense(1, activation='sigmoid'))  # Binary classification\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# 2. Train the BiLSTM model\n",
        "def train_bilstm_model(X_train, y_train, X_val, y_val):\n",
        "\n",
        "\n",
        "    input_shape = (None, X_train.shape[2])  # Shape: (sequence_length, embedding_dimension)\n",
        "\n",
        "    # Build the BiLSTM model\n",
        "    model = build_bilstm_model(input_shape)\n",
        "\n",
        "    # Print the model summary\n",
        "    model.summary()\n",
        "\n",
        "    # Train the model\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_val, y_val),\n",
        "        epochs=5,          # You can tune the number of epochs\n",
        "        batch_size=32,     # Adjust batch size based on available memory\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# 3. Evaluate the model on test data\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    # Predict on test data\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred = (y_pred > 0.5).astype(int)  # Convert probabilities to binary (0 or 1)\n",
        "\n",
        "    # Evaluate accuracy\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    print(f'Test Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Classification report and confusion matrix\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# Example Usage\n",
        "# Assuming you've already generated ELMo embeddings as X_train, X_val, X_test\n",
        "# And your labels are stored in y_train, y_val, y_test\n",
        "\n",
        "# Train the model\n",
        "bilstm_model, history = train_bilstm_model(X_train, y_train, X_val, y_val)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "evaluate_model(bilstm_model, X_test, y_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAVmmtexqNMZ",
        "outputId": "2e824b1e-6cc8-472b-cf68-dfa777710c8e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " bidirectional_10 (Bidirect  (None, 256)               1180672   \n",
            " ional)                                                          \n",
            "                                                                 \n",
            " dropout_10 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1180929 (4.50 MB)\n",
            "Trainable params: 1180929 (4.50 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "250/250 [==============================] - 24s 85ms/step - loss: 0.5164 - accuracy: 0.7408 - val_loss: 0.4678 - val_accuracy: 0.7800\n",
            "Epoch 2/5\n",
            "250/250 [==============================] - 21s 82ms/step - loss: 0.4107 - accuracy: 0.8139 - val_loss: 0.4040 - val_accuracy: 0.8210\n",
            "Epoch 3/5\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.3412 - accuracy: 0.8495 - val_loss: 0.4064 - val_accuracy: 0.8020\n",
            "Epoch 4/5\n",
            "250/250 [==============================] - 20s 82ms/step - loss: 0.2730 - accuracy: 0.8842 - val_loss: 0.4322 - val_accuracy: 0.8250\n",
            "Epoch 5/5\n",
            "250/250 [==============================] - 20s 81ms/step - loss: 0.1999 - accuracy: 0.9214 - val_loss: 0.4613 - val_accuracy: 0.8180\n",
            "52/52 [==============================] - 2s 30ms/step\n",
            "Test Accuracy: 83.27%\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.90      0.84       831\n",
            "           1       0.89      0.76      0.82       831\n",
            "\n",
            "    accuracy                           0.83      1662\n",
            "   macro avg       0.84      0.83      0.83      1662\n",
            "weighted avg       0.84      0.83      0.83      1662\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[751  80]\n",
            " [198 633]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bl_cATs4v3y5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}